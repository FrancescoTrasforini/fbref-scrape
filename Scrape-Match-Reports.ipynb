{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service\n",
    "from selenium.webdriver.edge.options import Options\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Selenium WebDriver\n",
    "def init_webdriver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    service = Service(EdgeChromiumDriverManager().install())\n",
    "    driver = webdriver.Edge(service=service, options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the page and extract HTML content\n",
    "def get_page_content(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(10)  # Wait for the page to load\n",
    "    html = driver.page_source\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_match_report_urls(html):\n",
    "    base_url = \"https://fbref.com\"  # Root URL to prepend to relative links\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"matchlogs_for\"})\n",
    "    \n",
    "    if not table:\n",
    "        print(\"Table not found.\")\n",
    "        return []\n",
    "\n",
    "    # Extract rows\n",
    "    rows = table.find(\"tbody\").findAll(\"tr\")\n",
    "    match_report_urls = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.findAll(\"td\")\n",
    "        if len(cells) > 0:\n",
    "            match_report_cell = cells[-2]\n",
    "            # Print cell HTML for debugging\n",
    "            print(f\"Cell HTML: {match_report_cell.prettify()}\")\n",
    "            \n",
    "            link = match_report_cell.find(\"a\")\n",
    "            relative_url = link[\"href\"] if link else None\n",
    "            match_report_url = base_url + relative_url\n",
    "            match_report_urls.append(match_report_url)\n",
    "            print(f\"Extracted URL: {match_report_url}\")  # Debugging line\n",
    "    \n",
    "    return match_report_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the main Excel file with Match Report URLs\n",
    "def update_match_report_urls(df, urls):\n",
    "    df[\"Match Report\"] = urls\n",
    "    df.to_excel(\"gwangju_fc_matches_2024_updated.xlsx\", index=False)\n",
    "    print(\"Updated main Excel file with Match Report URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gwangju_player_stats(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Find the table with the specific caption\n",
    "    table = None\n",
    "    for tbl in soup.find_all('table'):\n",
    "        caption = tbl.find('caption')\n",
    "        if caption and caption.get_text(strip=True) == \"Gwangju FC Player Stats Table\":\n",
    "            table = tbl\n",
    "            break\n",
    "    \n",
    "    if not table:\n",
    "        raise ValueError(\"Table with caption 'Gwangju FC Player Stats Table' not found.\")\n",
    "    \n",
    "    # Skip the first tr (header with unnecessary grouping)\n",
    "    header_rows = table.find('thead').find_all('tr')\n",
    "    \n",
    "    if len(header_rows) > 1:\n",
    "        # The second row contains the actual column names\n",
    "        actual_headers_row = header_rows[1]\n",
    "        headers = [header.text.strip() for header in actual_headers_row.find_all('th')]\n",
    "    else:\n",
    "        # If there's no second row, fallback to the first row\n",
    "        headers = [header.text.strip() for header in header_rows[0].find_all('th')]\n",
    "\n",
    "    # Extract data rows\n",
    "    rows = table.find('tbody').find_all('tr')\n",
    "    data = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = [cell.text.strip() for cell in row.find_all(['td', 'th'])]  # Handle both headers and data\n",
    "        \n",
    "        # Replace empty values with 0\n",
    "        cells = [cell if cell else '0' for cell in cells]  # <--- Replacement of empty data with '0'\n",
    "        \n",
    "        if cells:\n",
    "            data.append(cells)\n",
    "\n",
    "    # Adjust columns dynamically based on the data\n",
    "    num_columns = len(data[0]) if data else 0\n",
    "    if num_columns != len(headers):\n",
    "        # Handle cases where data and headers mismatch\n",
    "        print(f\"Warning: Mismatch between headers ({len(headers)}) and data columns ({num_columns}). Adjusting headers.\")\n",
    "        headers = headers[:num_columns]  # Truncate headers to match data if necessary\n",
    "\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the scraped match report table to a new Excel file\n",
    "def save_report(df, opponent, match_number):\n",
    "    folder_path = \"Match-Reports\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    \n",
    "    filename = f\"Report Gwangju FC - {opponent} - Matchday {match_number}.xlsx\"\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    df.to_excel(file_path, index=False)\n",
    "    print(f\"Saved match report to {file_path}\")\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape and save each match report as a new Excel file\n",
    "def scrape_and_save_reports(df):\n",
    "    driver = init_webdriver()\n",
    "    try:\n",
    "        for index, row in df.iterrows():\n",
    "            match_report_url = row[\"Match Report\"]\n",
    "            opponent = row[\"Opponent\"]\n",
    "            match_number = index + 1  # Match number as a unique identifier\n",
    "            \n",
    "            # Skip if there's no valid match report URL\n",
    "            if not pd.isna(match_report_url):\n",
    "                \n",
    "                # Check if the URL contains \"stathead\" and break if true\n",
    "                if \"stathead\" in match_report_url:\n",
    "                    print(f\"Skipping Match {match_number} (not yet played): {match_report_url}\")\n",
    "                    break  # Stop processing further rows\n",
    "\n",
    "                print(f\"Processing Match {match_number}: Gwangju FC vs {opponent}\")\n",
    "                \n",
    "                # Load match report page\n",
    "                html = get_page_content(driver, match_report_url)\n",
    "                match_report_df = extract_gwangju_player_stats(html)\n",
    "                \n",
    "                if match_report_df is not None:\n",
    "                    # Save the scraped table to a new Excel file\n",
    "                    file_path = save_report(match_report_df, opponent, match_number)\n",
    "                    \n",
    "                    # Optionally update the main Excel file with file paths or other metadata\n",
    "                    df.at[index, \"Match Report\"] = file_path\n",
    "                \n",
    "            else:\n",
    "                print(f\"No match report found for Match {match_number}\")\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the main Excel file\n",
    "    filename = \"gwangju_fc_matches_2024.xlsx\"\n",
    "    df = pd.read_excel(filename)\n",
    "\n",
    "    # Load the matchlogs_for page and extract URLs\n",
    "    url = \"https://fbref.com/en/squads/ae306ede/Gwangju-FC-Stats#all_matchlogs\"\n",
    "    driver = init_webdriver()\n",
    "    html = get_page_content(driver, url)\n",
    "    match_report_urls = extract_match_report_urls(html)\n",
    "    driver.quit()\n",
    "\n",
    "    # Update the main Excel file with URLs\n",
    "    update_match_report_urls(df, match_report_urls)\n",
    "\n",
    "    # Re-load the updated main Excel file with URLs\n",
    "    df = pd.read_excel(\"gwangju_fc_matches_2024_updated.xlsx\")\n",
    "\n",
    "    # Scrape and save each match report\n",
    "    scrape_and_save_reports(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NBA_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
